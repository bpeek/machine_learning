---
title: "LC_writeup"
author: "Brendan Peek"
date: "August 13, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
###I: Abstract

The goal of this project was to create a model that predicted whether a loan application would be accepted better than guessing. The model was built using Lending Club data for the first quarter of 2017. Because the response variable was binary logistic regression was used to estimate the model parameters. Since 93.5% of loan applications in the dataset were rejected this was the benchmark to measure the model against. By rejecting every loan application we could be right 93.5% of the time. The model succeeded in correctly classifying 94.6% of the loans as either accepted or rejected. This may not seem like much of a difference but with a volume of approximately 1.5 million loan applications the 1% improvement adds another 15,000 correct classifications beyond the naive approach of rejecting all applications.

###II: Introduction
The libraries needed for this analysis are readr to read the csv files, ggplot2 for plots and graphs, maps for the heatmap, and pscl for McFadden's pseudoR2.
```{r libraries, echo=TRUE}
library(readr)
library(ggplot2)
library(maps)
library(pscl)
library(knitr)
```

Next, we need to load the data downloaded from the Lending Club website.

```{r load_data, echo=FALSE}
RejectStats_2017Q1 <- read_csv("C:/Users/Brendan/Desktop/Datasets/Lending Club/RejectStats_2017Q1.csv")
LoanStats_2017Q1 <- read_csv("C:/Users/Brendan/Desktop/Datasets/Lending Club/LoanStats_2017Q1.csv")
```
Predictor Variables
    
* LoanAmt (Continuous): The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.
* Title (Categ.): The loan title provided by the borrower
* DebtToInc (Continuous): A ratio calculated using the borrower's total monthly debt 
* Zip (Categ.): The first 3 numbers of the zip code provided by the borrower in the loan application.
* State (Categ.): The state provided by the borrower in the loan application
* EmpLength: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. 

Dependent Var

* Accepted (Categ.): dummy variable = 1 if loan application was accepted and = 0 if it was rejected
  
  
Now we want to create a dataframe with just the variables we want and then remove the large file to free up memory.

```{r, echo=TRUE}
accepted.df <- data.frame(LoanStats_2017Q1$loan_amnt,
                          LoanStats_2017Q1$title,
                          LoanStats_2017Q1$dti,
                          LoanStats_2017Q1$zip_code,
                          LoanStats_2017Q1$addr_state,
                          LoanStats_2017Q1$emp_length)
colnames(accepted.df) <- c("LoanAmt", "Title", "DebtToInc","Zip","State","EmpLength")

remove(LoanStats_2017Q1)
```


We want to do the same with the rejected dataset.

```{r, echo=TRUE}
rejected.df <- data.frame(RejectStats_2017Q1$`Amount Requested`,
                          RejectStats_2017Q1$`Loan Title`,
                          RejectStats_2017Q1$`Debt-To-Income Ratio`,
                          RejectStats_2017Q1$`Zip Code`,
                          RejectStats_2017Q1$State,
                          RejectStats_2017Q1$`Employment Length`)
colnames(rejected.df) <- c("LoanAmt", "Title", "DebtToInc","Zip","State","EmpLength")

remove(RejectStats_2017Q1)
```

Now we have the accepted and rejected dataframes with the same variables and column names which will make it easy to combine them. The final step before combining them is to create a dummy variable which will be the dependent variable in the model. Then we merge the two dataframes, remove unneeded data, and save the resulting dataframe to a csv file.

```{r, echo=TRUE}
acc.dummy.vector <- rep(1,nrow(accepted.df))
rej.dummy.vector <- rep(0,nrow(rejected.df))

accepted.df["Accepted"] <- acc.dummy.vector
rejected.df["Accepted"] <- rej.dummy.vector

acc.dummy.vector <- rep(1,nrow(accepted.df))
rej.dummy.vector <- rep(0,nrow(rejected.df))

accepted.df["Accepted"] <- acc.dummy.vector
rejected.df["Accepted"] <- rej.dummy.vector

loans <- rbind(accepted.df,rejected.df)
#write.csv(loans, file = "C:/Users/Brendan/Desktop/Datasets/Lending Club/loans.csv")

remove(acc.dummy.vector, rej.dummy.vector, accepted.df, rejected.df)
```

###III: Single Variable Exploration
Before any model building we need to get to know the data. This step will give us an idea of how each variable is distributed, help us find problematic observations, and help us determine which variables are likely to be useful in building the model. 

We'll look at the data in the following order:

1. Loan Amount
2. Title
3. Debt to Income
4. Zip Code
5. State
6. Employment Length


#####Loan Amount
```{r, echo=TRUE}
ggplot(data = loans, aes(loans$LoanAmt))+
  geom_histogram(bins = 50, col = "grey", fill = "black") +
  labs(title = "Histogram of Loan Amount", x = "Loan Amount", y = "Count")+
  theme(plot.title = element_text(hjust = 0.5))
  
  
summary(loans$LoanAmt)

ggplot(data = loans, aes(x = 0,y=loans$LoanAmt))+
  geom_boxplot()+
  stat_boxplot(geom ='errorbar') +
  labs(title = "Boxplot of Loan Amount", x = "", y = "Loan Amount")+
  theme(plot.title = element_text(hjust = 0.5))
```
We can see from these summaries that the vast majority of loan applications are for amounts less than \$50,000.00 with 50% of loan applications for less than \$10,000

#####Title
```{r, echo=TRUE}
counts <- table(loans$Title)
c <- data.frame(counts)
colnames(c)<-c("Title","Freq")

ggplot(data = c, aes(x = reorder(Title, Freq), y= Freq))+
  geom_bar(stat = "identity")+
  labs(title = "Title Frequency Barchart", x = "Title", y = "Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

title.df <- data.frame(table(loans$Title))
colnames(title.df) <- c("Title", "Freq")
title.df
```
We can see from this output that many of the categories are only separate because of slightly different names and can therefor be combined to create more informative plots.

The changes to be made are:

* Business Loan, small_business $\rightarrow$ Business
* major_purchase, Major purchase $\rightarrow$ Major Purchase
* Car financing, car $\rightarrow$ Car 
* vacation -> Vacation
* medical, Medical expense $\rightarrow$ Medical
* Credit card refinancing, credit_card, Debt consolidation, debt_consolidation $\rightarrow$ Debt Consolidation
* other $\rightarrow$ Other
* Green loan, renewable_energy $\rightarrow$ Green Loan
* Home buying, house, home_improvement $\rightarrow$ Home Buying or Improvement
* Moving and relocation, moving $\rightarrow$ Moving
  
The following code will make the desired changes to the data and recreate the plot and summary from above with it.
```{r, echo=TRUE}
loans$Title <-gsub(".*usiness.*","Business", loans$Title)
loans$Title <-gsub(".*purchase.*","Major Purchase", loans$Title)
# need to be careful here because "car" shows up in "credit card" so I can't use the "all character" expression I used above
loans$Title <-gsub("Car financing","Car", loans$Title)
loans$Title <-gsub("car","Car", loans$Title)
loans$Title <-gsub("vacation","Vacation", loans$Title)
loans$Title <-gsub(".*edical.*","Medical", loans$Title)
loans$Title <-gsub(".*redit.*","Debt Consolidation", loans$Title)
loans$Title <-gsub(".*consoli.*","Debt Consolidation", loans$Title)
loans$Title <-gsub("other","Other", loans$Title)
loans$Title <-gsub("Green loan","Green Loan", loans$Title)
loans$Title <-gsub("renew.*","Green Loan", loans$Title)
loans$Title <-gsub("Home.*","Home Buying or Improvement", loans$Title)
loans$Title <-gsub("home_.*","Home Buying or Improvement", loans$Title)
loans$Title <-gsub("house","Home Buying or Improvement", loans$Title)
loans$Title <-gsub(".*oving.*","Moving", loans$Title)

title.df <- data.frame(table(loans$Title))
colnames(title.df) <- c("Title", "Freq")
title.df


ggplot(data = title.df, aes(x = reorder(Title, Freq), y= Freq))+
  geom_bar(stat = "identity")+
  labs(title = "Title Frequency Barchart", x = "Title", y = "Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
The table and barplot become much easier to understand and it it easy to see that the most popular reason by far for applying is debt consolidation. In fact, it is more popular than all the other reasons combined.

#####Debt to Income
The first thing to notice is that the debt to income data is stored as a character string. We will need to convert it to numeric.
```{r, echo=TRUE}
v <- as.numeric(loans$DebtToInc)
```
We can see that NAs were introduced. This is odd because the debt to income is a ratio so all the entries should easily convert to numeric. To get an idea of what the problem is we can find the first data point that results in NA and look at it.
```{r, echo=TRUE}
na.index <- is.na(v)
first.na <- min(which(na.index == TRUE))
loans$DebtToInc[first.na]
```
It looks like the entries for a debt to income ratio of zero have a percent sign which is causing the numeric conversion to fail. Hopefully this is the only problem. 

To fix it we will simply substitute an empty string for the "%" symbol in all the debt to income entries. After the substitution we can try the numeric conversion again and if there are no NAs we will assign the converted column to the loans dataframe.
```{r, echo=TRUE}
loans$DebtToInc <-gsub("%","", loans$DebtToInc)
v2 <- as.numeric(loans$DebtToInc)
sum(is.na(v2))
```
No NAs were produced by the conversion so we are ready to put the data into the main dataframe.
```{r, echo=TRUE}
loans$DebtToInc <- as.numeric(loans$DebtToInc)
```

With the data all cleaned up we can check it out.

```{r, echo=TRUE}
summary(data.frame(loans$DebtToInc))

ggplot(data = loans, aes(loans$DebtToInc))+
  geom_histogram(bins = 50, col = "grey", fill = "black") +
  labs(title = "Histogram of Debt to Income", x = "Debt to Income", y = "Count")+
  theme(plot.title = element_text(hjust = 0.5))

ggplot(data = loans, aes(x = 0,y=loans$DebtToInc))+
  geom_boxplot()+
  stat_boxplot(geom ='errorbar') +
  labs(title = "Boxplot of Debt to Income", x = "", y = "Debt to Income")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

The summary is the most informative of this output because the extreme values are causing the range on the plots to squeeze most of the data into a very small area. The summary shows that 75% of the applicants have debt to income ratios below 36%. The maximum value is suprising and may be an outlier. We will need to look into that below.

To get a better view at the bulk of the data without the influence of the outliers we can set the x axis limits to 100 for the histogram and the y limit to 100 for the boxplot.

```{r, echo=TRUE}
ggplot(data = loans, aes(loans$DebtToInc))+
  geom_histogram(bins = 50, col = "grey", fill = "black") +
  labs(title = "Histogram of Debt to Income", x = "Debt to Income", y = "Count")+
  theme(plot.title = element_text(hjust = 0.5))+
  xlim(-1,100)

ggplot(data = loans, aes(x = 0,y=loans$DebtToInc))+
  geom_boxplot()+
  stat_boxplot(geom ='errorbar') +
  labs(title = "Boxplot of Debt to Income", x = "", y = "Debt to Income")+
  theme(plot.title = element_text(hjust = 0.5))+
  ylim(-1,100)+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```
From these two plots we can see much more clearly that the bulk of applications have debt to income ratios less than 50.

As a first pass at it we can look for debt to income ratios above 1000% which seems like an abnormally high DTI.

```{r, echo=TRUE}
high.dti <- which(loans$DebtToInc > 1000)
length(high.dti)
```
It turns out that over 10,000 applicants have very high DTI ratios and the extreme values may not be mistakes after all.


#####Zip Code
Zip code may be too granular to be helpful in building the model but we should look at it just in case.
```{r, echo=TRUE}


zip.counts <- table(loans$Zip)
z <- data.frame(zip.counts)
colnames(z)<-c("Zip","Freq")
ggplot(data = z, aes(x = reorder(Zip, Freq), y= Freq))+
  geom_bar(stat = "identity")+
  labs(title = "Zip Code Frequency Barchart", x = "Zip Code", y = "Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x=element_blank())
#the x axis text was removed because there were so many values that they blurred together and were unreadable.
```
It isn't terribly helpful beyond pointing out that there are zip codes where Lending Club is much more popular than others. We can see this more clearly with a US heatmap which we will do below.

#####States
It is much clearer when we look at the number of loan applications at the state level.
```{r, echo=TRUE}


state.counts <- table(loans$State)
s <- data.frame(state.counts)
colnames(s)<-c("State","Freq")
ggplot(data = s, aes(x = reorder(State, Freq), y= Freq))+
  geom_bar(stat = "identity")+
  labs(title = "State Frequency Barchart", x = "State", y = "Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
# look at states with the most loan applications
tail(sort(state.counts))

# create a map of the US colored based on number of loan applications per state
state.df <- as.data.frame(state.counts)
colnames(state.df) <- c("State", "Freq")
state.names <- c("alaska","alabama", "arkansas","arizona","california","colorado","connecticut",
                 "district of columbia", "delaware","florida","georgia","hawaii","idaho","illinois",
                 "indiana","kansas","kentucky","louisiana","massachusetts", "maryland","maine","michigan", "minnesota",
                 "missouri","mississippi","montana","north carolina","north dakota","nebraska","new hampshire",
                 "new jersey", "new mexico","nevada","new york","ohio","oklahoma","oregon","pennsylvania",
                 "rhode island","south carolina","south dakota", "tennessee","texas","utah","virginia",
                 "vermont","washington","wisconsin","wyoming","west virginia")
state.df["region"] <- state.names

#Iowa is not in the dataset because there were no loan apps from that state
# to make it show up on the map we need to include it in the dataset as zero
ia.df <- data.frame(as.factor("IA"),0,"iowa")
names(ia.df) <- c("State","Freq","region")

#append it to the state.df dataframe
state.df <- rbind(state.df, ia.df)

states <- map_data("state")

ggplot(data = state.df, aes(map_id = region)) + 
  geom_map(aes(fill = Freq), map = states) +
  scale_fill_gradientn(colours=c("white","red","orange","yellow")) + 
  expand_limits(x = states$long, y = states$lat)+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(panel.background = element_rect(fill = 'grey', color = 'grey'))+
  theme(panel.border = element_rect(fill=NA, color = 'black'))+
  theme(axis.text = element_blank())+
  theme(axis.ticks = element_blank())+
  theme(panel.grid = element_blank())+
  labs(title = "State Frequency Map", x="",y="")
```
<br>We can see from the histogram that a few states account for the majority of applications. The top six states by number of applications from greatest to least are CA, TX, FL, NY, PA, GA. The heatmap of the US gives solidifies this with CA being the clear leader in yellow with applications numbering more than 150,000. The orange states of Texas, Florida, and New York are not far behind with around 100,000 applications each.


#####Employment Length
The employment length data needs a bit of cleaning before it can be converted into numeric form. 
```{r, echo=TRUE}
head(loans$EmpLength)
loans$EmpLength <- as.character(loans$EmpLength)
#the first step toward getting these as numeric values is to cut "years" of the strings in the column
loans$EmpLength <-gsub("years","", loans$EmpLength)
# now cut "year"
loans$EmpLength <-gsub("year","", loans$EmpLength)
# now cut "< 1" and replace it with 0
loans$EmpLength <-gsub("< 1","0", loans$EmpLength)
# make 10+ just 10
loans$EmpLength <-gsub("[+]","", loans$EmpLength)
# check to see if we have the values we want now
emp.length.table <- table(loans$EmpLength)
# find what row contains ".1" 
ind <- grepl(".1", loans$EmpLength)
which(ind == TRUE)
# ".1" is in row 109907
# I can't figure out why this is here but there are so many observations that I'm just going to delete this row
# and it won't effect the model in any significant way
loans <- loans[-c(109907), ]
# now I can convert the years of employment to numeric
# since the data is categorical I'll use a barplot and table to summarize
loans$EmpLength <- as.numeric(loans$EmpLength)
emp.length <- table(loans$EmpLength)
e <- data.frame(emp.length)
colnames(e)<-c("Years","Freq")

ggplot(data = e, aes(x = Years, y= Freq))+
  geom_bar(stat = "identity")+
  labs(title = "Employment Length Frequency Barchart", x = "Years", y = "Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 0, hjust = 1, size = 7))

# the great majority of applicants have been working for less than a year which leads me to believe that people
# come to lending club if they are not eligible for traditional credit

# the conversion warned of NAs but after checking the table of counts when it was char vs numeric
# i found that the only NAs were from the "n/a" char strings 
```
After taking a quick look at the first few entries in the Employment Length column we can see that the data is not stored in a format that is easily converted to numeric values. We need to remove everything except the numbers before conversion. Using gsub we remove the strings "years", "year", "<", and "+". Then we create a table of counts to see if we have the data in the form we want. There is one odd value in the table (0.1) and we don't know what caused it but since the dataset is so large we can just remove it from the dataframe. Finally, with the data cleaned up and converted to numeric values we create a histogram from which we can see that the vast majority of applicants have fewer than one year of employment in their current position or are unemployed.

###IV: Cleaning
Now for one of the most important pieces, to make sure the model has clean and reliable data to use. This will most entail removing NA values but it also includes finding and evaluating odd values.
```{r, echo=TRUE}
LoanAmt.na <- which(is.na(loans$LoanAmt))
length(LoanAmt.na)
```
We are off to a good start with no NAs in the Loan Amount column.

```{r, echo=TRUE}
Title.na <- which(is.na(loans$Title))
length(Title.na)
# there are 47 NAs in Title. I'll print the rows to see if there is any pattern.
loans[Title.na, ]
# these entries all had a zero loan amount, no title, -1% DTI, employment length < 1, and were rejected
# these entries seem to either be incomplete applications or faulty entries. They will be removed from the
# dataset
loans <- loans[-c(Title.na), ]
```
There are 47 NA values in the Title column. They all had a zero loan amount, no title, -1% Debt to Income, employment length less than 1 and were rejected. They seem to be either faulty entries or incomplete loan applications. In either case they should not be included in the model building dataset because we are only interested in the rejection or acceptance of completed valid loan applications. The 47 observations were removed from the dataset.

```{r, echo=TRUE}
dti.na <- which(is.na(loans$DebtToInc))
length(dti.na)
# there are no NAs but there are still odd values like -1% DTI to investigate
sum(loans$DebtToInc == -1)
# there are 112,046 applications with -1% DTI
dti.neg <- which(loans$DebtToInc == -1)
sum(loans$Accepted[dti.neg] == 1)
# only one of the 112,046 applications with -1% DTI was accepted 
summary(loans[dti.neg, ])
# the data for applicants with -1% DTI is quite varied except for the fact that they were almost all rejected
# this may be a valuable predictor even if I can't figure out what it means. I wonder if it means that the 
# applicant has no reported income and therefore has an undefined DTI ratio. I'll leave it in.
```
There are no NAs in the Debt to Income column but there are many -1 values which don't make sense because the ratio values should be between 0 and infinity. The first thing we find is that -1 values are very common. There are 112,046 applications with this value but only one of them was accepted. This may be a default value if the applicant doesn't fill it in. We next look at the summary of the data to get an idea of its distribution. The data for applications with this value are quite varied so it may be a good predictor even if we don't know exactly what it means.

We will not be including zip codes in the model because we don't want 900+ dummy variables. So, we don't need to clean the data.

Next, we'll check the State column.
```{r, echo=TRUE}
state.na <- which(is.na(loans$State))
length(state.na)
# no NAs in State
```
There are no NAs in this data so we can move on down the line to Employment Length.

```{r, echo=TRUE}
emp.na <- which(is.na(loans$EmpLength))
length(emp.na)
# there are 74623 NAs in the Employment Length column
summary(loans[emp.na, ])
summary(loans)
length(emp.na)/length(loans$EmpLength)
# the data with EmpLength equal to NA looks similarly distributed to the dataset as a whole and it makes up
# only 5% of the observations so I'm going to remove it from the dataset.
loans <- loans[-c(emp.na), ]
# check that the rows were removed
sum(which(is.na(loans$EmpLength)))

```
We find that there are 74,623 NA values in the Employment Length column but it looks like it is similarly distributed to the dataset as a whole and it is only about 5% of the total data so we will remove it.

The final variable to check is Accepted which should not have any NA values because we created it but it's better to check just in case.

```{r, echo=TRUE}

sum(which(is.na(loans$Accepted)))

```
As expected, there are no NA values in the Accepted column.

Just because we want to be as sure as possible that the data has been thoroughly cleaned we will check the whole dataset one more time.
```{r, echo=TRUE}

colSums(is.na(loans))

```
Happily, none of the variables for our model have NAs left. Now we can move on to comparing the dependent variable to each of the explanatory variables in what we'll call "Bivariate Exploration!!!!".

###V: Bivariate Exploration
Here we will look at the relationship between the chosen explanatory variables and the dependent variable.

Continuous explanatory vars:
  
* LoanAmt
* DebtToInc
* EmpLength

Categorical explanatory vars:

* Title
* State

Dependent var:

* Accepted

#####Loan Amount vs Accepted
```{r, echo=TRUE}
by(data.frame(loans$LoanAmt), data.frame(loans$Accepted), summary)
options(scipen=10000)
ggplot(data = loans, aes(x = as.factor(Accepted), y=LoanAmt))+
  geom_boxplot()+
  stat_boxplot(geom ='errorbar') +
  labs(title = "Loan Amount Boxplot", x = "Accepted", y = "Loan Amount")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 0, hjust = 1))
# this seems to indicate that loan amounts > 50,000 are very unlikely (if not impossible) to get funded
```
We can see from the summary that the accepted loans vary much less than the rejected loans. It looks like it is very unlikely if not impossible for a loan above \$40,000 to be accepted. Through further exploration with boxplots we can see that the majority of both accepted and rejected loan applications are below \$50,000 but the rejected applications have many more extreme values.

#####Debt to Income vs Accepted
```{r, echo=TRUE}

by(loans$DebtToInc, loans$Accepted, summary)
ggplot(data = loans, aes(x = as.factor(Accepted), y=DebtToInc))+
  geom_boxplot()+
  stat_boxplot(geom ='errorbar') +
  labs(title = "Debt to Income Boxplot", x = "Accepted", y = "Debt to Income")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 0, hjust = 1))
# the outliers in the rejected loan requests are so extreme that the plot is effectively useless.
# the summary of DTI grouped by accepted show a wider spread and much higher extreme values in the rejected
# to get a better look I'll plot them again, ignoring the extreme values
ggplot(data = loans, aes(x = as.factor(Accepted), y=DebtToInc))+
  geom_boxplot()+
  stat_boxplot(geom ='errorbar') +
  labs(title = "Debt to Income Boxplot", x = "Accepted", y = "Debt to Income")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 0, hjust = 1))+
  ylim(0,100)
# this shows that the DTI range on accepted loan applications is much narrower and tends to be lower than the 
# rejects. This makes sense. Lower DTI indicates a greater ability to repay the loan because they aren't 
# spending all their money servicing other debt.
```
The first look at the summary finds odd values in both the accepted and rejected data. The max debt to income ratio in the rejected applications is 559000% and the max in the accepted applications is 9999%. These may be mistakes or extreme outliers but we can be comforted by the fact that 75% of applications in both rejected and accepted have more sensible debt to income ratios less than 37% and 25% respectively. 

Looking at the boxplots we can see that the extreme values in  the rejected data make the plots effectively useless. To get a better view of the data we can exclude the extreme values and plot again. This time we can see that the accepted range is much narrower that the rejected range and that most of the accepted loans have debt to income ratios less than 50%. This makes sense because a lender is not going to want someone spending the majority of their income servicing debt each month. It's too risky.

#####Employment Length vs Accepted
```{r, echo=TRUE}
by(data.frame(loans$EmpLength), data.frame(loans$Accepted), summary)

ggplot(data = loans, aes(x = as.factor(Accepted), y=EmpLength))+
  geom_boxplot()+
  labs(title = "Employment Length Boxplot", x = "Accepted", y = "Years")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 0, hjust = 1))
```
These results should come as no suprise. The majority of rejected applicants have less than 1 year of employment or are unemployed while the majority of accepted applicants have 3 or more years of employment in their current position. Employment length doesn't explain all of it though. The some of the rejected applicants are employed and have been for up to 10 or more years. This makes sense because there are other factors that go into the lending decision besides how long a person has been employed.

#####Title vs Accepted
Now we are moving to categorical vs categorical comparisons so we will be using stacked bar plots instead of boxplots to compare.
```{r, echo=TRUE}
title.accept.counts <- table(loans$Accepted, loans$Title)
t <- data.frame(title.accept.counts)
colnames(t) <- c("Accepted", "Title", "Freq")
ggplot(data = t, aes(x = reorder(Title,Freq) , y= Freq, fill = Accepted))+
  geom_bar(stat = "identity")+
  labs(title = "Acceptance by Title", x = "Title", y = "Frequency")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
It looks like debt consolidation, home improvement and buying, and other are the only categories with significant acceptance rates. It might be more informative if we look at the ratios of acceptance for each title instead.
```{r, echo=TRUE}
title.sums <- by(loans$Accepted, loans$Title, sum)
title.len <- by(loans$Accepted, loans$Title, length)
title.rate <- title.sums/title.len

#get names and rates into a dataframe for ggplot
names <- c()
values <- c()
for (i in 1:length(title.rate)){
  df <- data.frame(title.rate[i])
  names[i] <- rownames(df)
  values[i] <- df[1,1]
}
title.df <- data.frame(names,values)
title.df

ggplot(data = title.df, aes(x = reorder(names, values), y= values))+
  geom_bar(stat = "identity")+
  labs(title = "Acceptance Rate by Title", x = "Title", y = "Rate")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
This does give a better idea of what purpose is most likely to be accepted. We see that the top three are debt consolidation, home buying or improvement, and vacation. The stacked bar plot didn't show this because vacation is such a small proportion of the applications that the ~4% that were accepted cannot be seen.

#####State vs Accepted
```{r, echo=TRUE}
state.sums <- by(loans$Accepted, loans$State, sum)
state.len <- by(loans$Accepted, loans$State, length)
state.rate <- state.sums/state.len
#barplot(sort(state.rate))
#sort(state.rate, decreasing = TRUE)
#summary(state.rate)

#get names and rates into a dataframe for ggplot
names <- c()
values <- c()
for (i in 1:length(state.rate)){
  df <- data.frame(state.rate[i])
  names[i] <- rownames(df)
  values[i] <- df[1,1]
}
state.df <- data.frame(names,values)
state.df

ggplot(data = state.df, aes(x = reorder(names, values), y= values))+
  geom_bar(stat = "identity")+
  labs(title = "Acceptance Rate by State", x = "State", y = "Rate")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))
```
The states with the highest rates are Connecticut, Vermont, Minnesota, New Hampshire, Colorado, and Oregon. None of them are the states with the highest number of applications but it would be interesting to see if they have a higher per capita application rate than other states. We can also see that most of the states have acceptance rates above 5% but that there is at least one with no accepted loan applications. After looking at the acceptance rates for all states we see that West Virginia is the only state with a rate of 0%.

All the explanatory variables appear to have a relationship to the dependent variable. Next we will split the data into a training set and a test set. Then we can build and evaluate the model.

###VI: Model Building
First, we want to split the data into a training set for estimating the model and a test set for testing the model's predictive ability.
```{r, echo=TRUE}
#choose random set of indices to use as training set (about 10% of observations)
test.ind <- sample(1:length(loans$Accepted), 140000, replace = FALSE)
test.data <- loans[test.ind, ]
train.data <- loans[-test.ind, ]
```
Then, we will estimate the logistic regression model with all the explanatory variables in it.
```{r, echo=TRUE}
accept.model <- glm(Accepted ~ Title+DebtToInc+State+EmpLength, data = train.data, family = binomial(link = 'logit'))
summary(accept.model)
```
```{r, echo=TRUE}
round(pR2(accept.model), 2)
```
The signs on Debt to Income and Employment Length are what we would expect (negative and positive, respectively). The different Title options have different signs with the best option being Debt Consolidation. 

The model is ugly and McFadden's pseudo R2 is 0.32 which is not great. Most of the states are not significant so we may be better off removing them. The last thing we need to check is the predictive ability of the model.

```{r, echo=TRUE}
baseline = sum(test.data$Accepted == 0)/length(test.data$Accepted)
baseline
# this set the bar pretty high because the model will have to beat 93.5% accuracy
fitted.results <- predict(accept.model, newdata = test.data)
fitted.results <- ifelse(fitted.results > 0.5, 1, 0)

correct.classification.rate <- mean(fitted.results == test.data$Accepted)
correct.classification.rate
```
First, we set a baseline where we reject all applications. Since most of the applications are rejected we end up being right 93.5% of the time. The model will need to beat this high bar to be useful. 

The model predicts the correct classification 94.57% of the time so it is doing better than baseline. This may not seem like much but we are dealing with about 1.5 million applications so a 1% improvement leads to about 15,000 more correct classifications.

The above model may be overly complicated. We will now build a model without the States variable and check its vitals to compare it to the more complicated model above.

```{r, echo=TRUE}
simp.accept.model <- glm(Accepted ~ Title+DebtToInc+EmpLength, data = train.data, family = binomial(link = 'logit'))
summary(simp.accept.model)
round(pR2(simp.accept.model), 2)

fitted.results2 <- predict(simp.accept.model, newdata = test.data)
fitted.results2 <- ifelse(fitted.results2 > 0.5, 1, 0)

correct.classification.rate2 <- mean(fitted.results2 == test.data$Accepted)
correct.classification.rate2
```
The simpler model is much easier to look at. All the explanatory variables are very significant and the pseudo R2 is the same as the more complicated model. The best part is that this model is slightly better at prediction. The complicated model correctly classified 94.57% of observations and the simple model correctly classified 94.60%.

###V: Conclusion
This model may be useful for people thinking about applying for a loan. They could decide whether it is worth the time to apply based on whether they expect to be accepted or not. It could also be used to explain lending decisions to regulators to ensure them that lending laws are being followed.

The next model we could build would use the accepted loan data to try and predict who will default once they have a loan. This could be used to better filter borrowers in the future.




